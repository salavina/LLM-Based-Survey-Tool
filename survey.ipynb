{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import gradio as gr \n",
    "import time\n",
    "import whisper\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = \"key.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_TYPE  = os.getenv('OPENAI_API_TYPE')\n",
    "OPENAI_API_BASE = os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_VERSION = os.getenv('OPENAI_API_VERSION')\n",
    "DEPLOYMENT_NAME = os.getenv('DEPLOYMENT_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ques_gen_pipeline = AzureChatOpenAI(\n",
    "    temperature = 0,\n",
    "    openai_api_base=OPENAI_API_BASE,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_type=OPENAI_API_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = { \"0\": {\n",
    "    \"text\": \"Hi, I hope you're doing well!\",\n",
    "    \"type\": \"answer should be either yes or no\",\n",
    "},\"1\": {\n",
    "    \"text\": \"I am very happy to see you here. I understand that after a stroke, it might feel like you've lost a piece of your life as you knew it, and it's entirely normal to experience a whirlwind of emotions like shock, denial, anger, grief, and guilt. Navigating through these feelings can be tough, but acknowledging them and seeking support can make a significant difference. I would love to hear from you, whether you have experienced any sad feelings or stress in the past few days?\",\n",
    "    \"type\": \"answer should be either yes or no\",\n",
    "},\"2\": {\n",
    "    \"text\": \"I am sorry you have experienced such feelings. How often would you say you have experienced such feelings?\",\n",
    "    \"type\": \"answer should be an interger between 1 and 5\",\n",
    "}, \"3\": {\n",
    "    \"text\": \"oh I am sorry, would you like to share with me in more detail some of the times that you experienced such stressful feelings so I can better provide guidance?\",\n",
    "    \"type\": \"answer is a text. e.g. I felt stressed due to multitasking during meal preparation for my children. first look through the answer and make sure there is no mistake in text to speech recognition. At the end provide the answer in the format of trigger: thought: feeling: behavior:\",\n",
    "}, \"4\": {\n",
    "    \"text\": \"Please confirm if I have correctly captured your situation in a helpful manner that would allow us to use a reflective stress management technique?\",\n",
    "    \"type\": \"answer should be either yes or no\",\n",
    "}, \"5\": {\n",
    "    \"text\": \"What is an alternetive line of thinking you would propose that can provoke more positive feelings and behavior?\",\n",
    "    \"type\": \"provide suggestions for the user based on their previous answer. Also, provide some useful links at the end as well.\",\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only question generation\n",
    "prompt_template = \"\"\"\n",
    "You are interviewing a post stroke patient and asking a series of questions.\n",
    "here is the question we ask the user:\n",
    "------------\n",
    "{question}\n",
    "------------\n",
    "The patient has difficulty speaking and we are using a speech to text model to get the user's response. Look through the answer and make sure there is no mistake in text to speech recognition.\n",
    "The expected output format is {type} so convert the answer to the proper output format based on users response.\n",
    "Here is the answer we received from interviewee:\n",
    "------------\n",
    "{answer}\n",
    "------------\n",
    "REFINED ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=[\"question\", \"type\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the text with GPT-4\n",
    "def analyze(llm, prompt, question, type, answer = ''):\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    refined_answer  = chain.run({'question': question, 'type': type, 'answer': answer})\n",
    "    return refined_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech to text\n",
    "def inference(audio):\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    model = whisper.load_model(\"tiny.en\")\n",
    "    result = model.transcribe(audio)\n",
    "\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaeidAlaviNaeini\\AppData\\Local\\Temp\\ipykernel_31776\\4141876674.py:53: GradioUnusedKwargWarning: You have unused kwarg parameters in Interface, please remove them: {'initial_value': '{\\'text\\': \"I am very happy to see you here. I understand that after a stroke, it might feel like you\\'ve lost a piece of your life as you knew it, and it\\'s entirely normal to experience a whirlwind of emotions like shock, denial, anger, grief, and guilt. Navigating through these feelings can be tough, but acknowledging them and seeking support can make a significant difference. I would love to hear from you, whether you have experienced any sad feelings or stress in the past few days?\", \\'type\\': \\'answer should be either yes or no\\'}'}\n",
      "  iface = gr.Interface(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7930\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7930/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\gradio\\processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\whisper\\transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\gradio\\processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\whisper\\transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\gradio\\processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\whisper\\transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\gradio\\processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "c:\\Users\\SaeidAlaviNaeini\\Projects\\survey\\env\\Lib\\site-packages\\whisper\\transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter to keep track of the question number\n",
    "question_counter = [0]\n",
    "ans = ''\n",
    "def survey_bot(user_response):\n",
    "    \n",
    "    \n",
    "    if question_counter[0] == 0:\n",
    "        # Bot's turn to ask a question\n",
    "        ans = questions[str(question_counter[0])]['text']\n",
    "        question_counter[0] += 1\n",
    "        return ans\n",
    "    \n",
    "    elif question_counter[0] == 1:\n",
    "        # Bot's turn to ask a question\n",
    "        ans = questions[str(question_counter[0])]['text']\n",
    "        question_counter[0] += 1\n",
    "        return ans\n",
    "    \n",
    "    elif question_counter[0] == 2:\n",
    "        # Bot's turn to ask a question\n",
    "        user_response = inference(user_response)\n",
    "        ans = analyze(llm_ques_gen_pipeline, PROMPT_QUESTIONS, questions[str(question_counter[0]-1)]['text'], questions[str(question_counter[0]-1)]['type'], user_response)\n",
    "        question_counter[0] += 1\n",
    "        return f\"Seems like you responded as {ans}, {questions[str(question_counter[0]-1)]['text']}\"\n",
    "    \n",
    "    elif question_counter[0] == 3:\n",
    "        # Bot's turn to ask a question\n",
    "        user_response = inference(user_response)\n",
    "        ans = analyze(llm_ques_gen_pipeline, PROMPT_QUESTIONS, questions[str(question_counter[0]-1)]['text'], questions[str(question_counter[0]-1)]['type'], user_response)\n",
    "        question_counter[0] += 1\n",
    "        return f\"So on scale of 1-5, your stress level has been {ans}, {questions[str(question_counter[0]-1)]['text']}\"\n",
    "    \n",
    "    elif question_counter[0] == 4:\n",
    "        # Bot's turn to ask a question\n",
    "        global ans_imp\n",
    "        ans_imp = inference(user_response)\n",
    "        user_response = ans_imp\n",
    "        ans = analyze(llm_ques_gen_pipeline, PROMPT_QUESTIONS, questions[str(question_counter[0]-1)]['text'], questions[str(question_counter[0]-1)]['type'], user_response)\n",
    "        question_counter[0] += 1\n",
    "        return f\"{questions[str(question_counter[0]-1)]['text']}: \\n\\n {ans}\"\n",
    "    \n",
    "    else:\n",
    "        user_response = inference(user_response)\n",
    "        ans = analyze(llm_ques_gen_pipeline, PROMPT_QUESTIONS, questions[str(question_counter[0]-2)]['text'], questions[str(question_counter[0]-1)]['type'], user_response)\n",
    "        ans1 = analyze(llm_ques_gen_pipeline, PROMPT_QUESTIONS, questions[str(question_counter[0])]['text'], questions[str(question_counter[0])]['type'], ans_imp)\n",
    "        # question_counter[0] += 1\n",
    "        return f\"Seems like you responded as {ans}, {ans1}\"\n",
    "    \n",
    "\n",
    "    # Increment the counter to prepare for the next question\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=survey_bot,                         # Function to be called\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"microphone\", type=\"filepath\")],                       # Type of input\n",
    "    outputs=\"text\",                        # Type of output                             # Enable live updates\n",
    "    title=\"Post Stroke Assessment Tool\",   # Description of the interface\n",
    "    initial_value=f\"{questions['1']}\",       # Initial value to display\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
